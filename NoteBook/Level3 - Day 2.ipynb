{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /home/labuser/Documents/Level3/Day2/transactions_partitions/*transactions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"CheckPartitions\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files from the partition\n",
    "#transactions_df = spark.read.csv(\"/home/labuser/Documents/Level3/Day2/transactions_partitions/\", header=True, inferSchema=True)\n",
    "transactions_df = spark.read.option(\"header\",True).option(\"inferSchema\", True).csv(\"/home/labuser/Documents/Level3/Day2/transactions_partitions/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of partitions\n",
    "num_partiotions_before = transactions_df.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions before optimization: {num_partiotions_before}\")\n",
    "\n",
    "# Get partition size\n",
    "partition_sizes_before = transactions_df.rdd.glom().map(len).collect()\n",
    "print(f\"Partition sizes before optimization: {partition_sizes_before}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Applying Repartition based on \"amount\"\n",
    "optimized_df = transactions_df.repartition(100, \"amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions_after = optimized_df.rdd.getNumPartitions()\n",
    "partition_sizes_after = optimized_df.rdd.glom().map(len).collect()\n",
    "\n",
    "print(f\"Number of partitions after optimization: {num_partitions_after}\")\n",
    "print(\"Partition sizes after optimization:\", partition_sizes_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD\n",
    "rdd = spark.sparkContext.parallelize(range(0, 100), numSlices=5)\n",
    "rdd.setName(\"rdd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.persist(StorageLevel.MEMORY_ONLY)\n",
    "print(\"RDD Count:\", rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF\n",
    "data = [(1, \"Manoj\"), (2, \"Mannu\"), (2, \"Ayushi\")]\n",
    "df = spark.createDataFrame(data, [\"count\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.persist(StorageLevel.DISK_ONLY)\n",
    "print(\"DataFrame count:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceGlobalTempView(\"df\")\n",
    "\n",
    "result = spark.sql(\"SELECT name, SUM(count) AS total_count FROM global_temp.df GROUP BY name\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklisted_df = spark.read.csv(\"/home/labuser/Documents/Level3/Day2/blacklisted_accounts.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transactions_df and blacklisted_df\n",
    "\n",
    "fraud_df = transactions_df.join(blacklisted_df, \"customer_id\", \"inner\")\n",
    "fraud_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "fraud_df = transactions_df.join(broadcast(blacklisted_df), \"customer_id\", \"inner\") # (<100MB)\n",
    "fraud_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WithSerialization\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"WithoutSerialization\").getOrCreate()\n",
    "\n",
    "# Create a simple banking transactions dataset\n",
    "data = [(1, \"Alice\", 1000.0), (2, \"Bob\", 2000.0), (3, \"Charlie\", 500.0)]\n",
    "df = spark.createDataFrame(data, [\"customer_id\", \"name\", \"balance\"])\n",
    "\n",
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform a simple transformation\n",
    "df = df.withColumnRenamed(\"balance\", \"account_balance\")\n",
    "df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time (Without Serialization):\", round(end_time - start_time, 4), \"seconds\")\n",
    "\n",
    "# Stop Spark session\n",
    "#spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------------+\n",
      "|customer_id|   name|account_balance|\n",
      "+-----------+-------+---------------+\n",
      "|          1|  Alice|         1000.0|\n",
      "|          2|    Bob|         2000.0|\n",
      "|          3|Charlie|          500.0|\n",
      "+-----------+-------+---------------+\n",
      "\n",
      "Execution Time (Without Serialization): 3.2292 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"WithoutSerialization\").getOrCreate()\n",
    "\n",
    "# Create a simple banking transactions dataset\n",
    "data = [(1, \"Alice\", 1000.0), (2, \"Bob\", 2000.0), (3, \"Charlie\", 500.0)]\n",
    "df = spark.createDataFrame(data, [\"customer_id\", \"name\", \"balance\"])\n",
    "\n",
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform a simple transformation\n",
    "df = df.withColumnRenamed(\"balance\", \"account_balance\")\n",
    "df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time (Without Serialization):\", round(end_time - start_time, 4), \"seconds\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------------+\n",
      "|customer_id|   name|account_balance|\n",
      "+-----------+-------+---------------+\n",
      "|          1|  Alice|         1000.0|\n",
      "|          2|    Bob|         2000.0|\n",
      "|          3|Charlie|          500.0|\n",
      "+-----------+-------+---------------+\n",
      "\n",
      "Execution Time (With Serialization): 1.1459 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Initialize Spark session with Kryo Serialization\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WithSerialization\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a simple banking transactions dataset\n",
    "data = [(1, \"Alice\", 1000.0), (2, \"Bob\", 2000.0), (3, \"Charlie\", 500.0)]\n",
    "df = spark.createDataFrame(data, [\"customer_id\", \"name\", \"balance\"])\n",
    "\n",
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform the same transformation\n",
    "df = df.withColumnRenamed(\"balance\", \"account_balance\")\n",
    "df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Execution Time (With Serialization):\", round(end_time - start_time, 4), \"seconds\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 3 ",
   "language": "python",
   "name": "pyspark3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
