{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/home/labuser/Documents/Level3/Day3/DataDemo/customers.parquet\n",
    "#/home/labuser/Documents/Level3/Day3/DataDemo/bank_loans.parquet\n",
    "#/home/labuser/Documents/Level3/Day3/DataDemo/branches.parquets\n",
    "#/home/labuser/Documents/Level3/Day3/DataDemo/transactions.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Converting timestamp from timestamp[ns] → TIMESTAMP(MILLIS)\n",
      "✅ TIMESTAMP(NANOS) successfully converted to TIMESTAMP(MILLIS)\n",
      "Saved as: /home/labuser/Documents/Level3/Day3/DataDemo/transactions_fixed.parquet\n",
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|      10222|    3|\n",
      "|      10371|    4|\n",
      "|      11193|    2|\n",
      "|      11378|    4|\n",
      "|      11553|    4|\n",
      "|      11568|    2|\n",
      "|      11664|    5|\n",
      "|      12285|    3|\n",
      "|      12829|    8|\n",
      "|      12830|    1|\n",
      "|      13304|    6|\n",
      "|      13670|    2|\n",
      "|      13687|    3|\n",
      "|      13694|    3|\n",
      "|      14144|    4|\n",
      "|      14846|    4|\n",
      "|      15901|    4|\n",
      "|      15953|    4|\n",
      "|      16568|    7|\n",
      "|      16749|    6|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time : 5.82 seconds\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"FixTimestampsAndJoin\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")  # Disable auto broadcast joins\n",
    "\n",
    "# Define file paths\n",
    "input_parquet = \"/home/labuser/Documents/Level3/Day3/DataDemo/transactions.parquet\"\n",
    "output_parquet = \"/home/labuser/Documents/Level3/Day3/DataDemo/transactions_fixed.parquet\"\n",
    "\n",
    "# Read the original Parquet file\n",
    "table = pq.read_table(input_parquet)\n",
    "\n",
    "# Convert TIMESTAMP(NANOS) to TIMESTAMP(MILLIS)\n",
    "new_columns = []\n",
    "for col_name, col_type in zip(table.column_names, table.schema.types):\n",
    "    if col_name == \"timestamp\" and pa.types.is_timestamp(col_type):\n",
    "        print(f\"⚠️ Converting {col_name} from {col_type} → TIMESTAMP(MILLIS)\")\n",
    "        \n",
    "        # Convert each chunk individually\n",
    "        timestamp_chunks = []\n",
    "        for chunk in table[col_name].chunks:\n",
    "            int_chunk = chunk.cast(pa.int64())  # Convert timestamp to int64 (nanoseconds)\n",
    "            millis_chunk = pa.array(int_chunk.to_numpy() // 1_000_000, type=pa.int64())  # Convert ns → ms\n",
    "            timestamp_chunks.append(millis_chunk.cast(pa.timestamp(\"ms\")))  # Cast back to TIMESTAMP(MILLIS)\n",
    "        \n",
    "        # Reconstruct column as a single ChunkedArray\n",
    "        new_columns.append(pa.chunked_array(timestamp_chunks))\n",
    "    else:\n",
    "        new_columns.append(table[col_name])\n",
    "\n",
    "# Create new table with updated schema\n",
    "fixed_table = pa.Table.from_arrays(new_columns, names=table.column_names)\n",
    "\n",
    "# Write the corrected Parquet file\n",
    "pq.write_table(fixed_table, output_parquet)\n",
    "\n",
    "print(\"✅ TIMESTAMP(NANOS) successfully converted to TIMESTAMP(MILLIS)\")\n",
    "print(\"Saved as:\", output_parquet)\n",
    "\n",
    "# ==============================\n",
    "# Load into PySpark and Perform Joins\n",
    "# ==============================\n",
    "\n",
    "# Read converted transactions\n",
    "transactions = spark.read.parquet(output_parquet)\n",
    "\n",
    "# Read customers data and rename \"CustomerID\" to \"customer_id\" for join compatibility\n",
    "customers = spark.read.parquet(\"/home/labuser/Documents/Level3/Day3/DataDemo/customers.parquet\") \\\n",
    "    .withColumnRenamed(\"CustomerID\", \"customer_id\")\n",
    "\n",
    "# Start Timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform Join (handling skewed data)\n",
    "transactions.join(customers, \"customer_id\", \"inner\").groupBy(\"customer_id\").count().show()\n",
    "\n",
    "# End Timerend\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate Execution time \n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution Time : {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|      10222|    3|\n",
      "|      10371|    4|\n",
      "|      11193|    2|\n",
      "|      11378|    4|\n",
      "|      11553|    4|\n",
      "|      11568|    2|\n",
      "|      11664|    5|\n",
      "|      12285|    3|\n",
      "|      12829|    8|\n",
      "|      12830|    1|\n",
      "|      13304|    6|\n",
      "|      13670|    2|\n",
      "|      13687|    3|\n",
      "|      13694|    3|\n",
      "|      14144|    4|\n",
      "|      14846|    4|\n",
      "|      15901|    4|\n",
      "|      15953|    4|\n",
      "|      16568|    7|\n",
      "|      16749|    6|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time : 1.03 seconds\n"
     ]
    }
   ],
   "source": [
    "# After Optimized Skew Joins & Bloom Filter Pushdown\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.optimizer.runtime.bloomfileter.enabled\", \"true\")\n",
    "\n",
    "# Start Timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform Join (handling skewed data)\n",
    "transactions.join(customers, \"customer_id\", \"inner\").groupBy(\"customer_id\").count().show()\n",
    "\n",
    "# End Timerend\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate Execution time \n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution Time : {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-10-20.ap-south-1.compute.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FixTimestampsAndJoin</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe8799777c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 3 ",
   "language": "python",
   "name": "pyspark3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
